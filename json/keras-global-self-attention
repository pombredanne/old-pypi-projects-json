{
    "info": {
        "author": "CyberZHG",
        "author_email": "CyberZHG@gmail.com",
        "bugtrack_url": null,
        "classifiers": [
            "License :: OSI Approved :: MIT License",
            "Operating System :: OS Independent",
            "Programming Language :: Python :: 2",
            "Programming Language :: Python :: 3"
        ],
        "description": "Keras Global Self-Attention\n===========================\n\n\n.. image:: https://travis-ci.org/CyberZHG/keras-global-self-attention.svg\n   :target: https://travis-ci.org/CyberZHG/keras-global-self-attention\n   :alt: Travis\n\n\n.. image:: https://coveralls.io/repos/github/CyberZHG/keras-global-self-attention/badge.svg?branch=master\n   :target: https://coveralls.io/github/CyberZHG/keras-global-self-attention\n   :alt: Coverage\n\n\nAttention mechanism for processing sequence data that considers the global context for each timestamp.\n\n\n* \n  .. image:: https://camo.githubusercontent.com/1ef0269557ea05b96b6894de202a109f6947dca6/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f685f253742742c2673706163653b74272537442673706163653b3d2673706163653b25354374616e6828785f74253545542673706163653b575f742673706163653b2b2673706163653b785f2537427427253744253545542673706163653b575f782673706163653b2b2673706163653b625f7429\n     :target: https://camo.githubusercontent.com/1ef0269557ea05b96b6894de202a109f6947dca6/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f685f253742742c2673706163653b74272537442673706163653b3d2673706163653b25354374616e6828785f74253545542673706163653b575f742673706163653b2b2673706163653b785f2537427427253744253545542673706163653b575f782673706163653b2b2673706163653b625f7429\n     :alt: \n\n* \n  .. image:: https://camo.githubusercontent.com/f8c64f2abd4752037c50deb7373b55362d7c51dc/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f655f253742742c2673706163653b74272537442673706163653b3d2673706163653b2535437369676d6128575f612673706163653b685f253742742c2673706163653b74272537442673706163653b2b2673706163653b625f6129\n     :target: https://camo.githubusercontent.com/f8c64f2abd4752037c50deb7373b55362d7c51dc/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f655f253742742c2673706163653b74272537442673706163653b3d2673706163653b2535437369676d6128575f612673706163653b685f253742742c2673706163653b74272537442673706163653b2b2673706163653b625f6129\n     :alt: \n\n* \n  .. image:: https://camo.githubusercontent.com/c63a13424300fe05bee615ce051fece8b5bc1c9a/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f615f253742742537442673706163653b3d2673706163653b25354374657874253742736f66746d617825374428655f7429\n     :target: https://camo.githubusercontent.com/c63a13424300fe05bee615ce051fece8b5bc1c9a/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f615f253742742537442673706163653b3d2673706163653b25354374657874253742736f66746d617825374428655f7429\n     :alt: \n\n* \n  .. image:: https://camo.githubusercontent.com/b9999104eccdcc594abbbef429a3fa49bac27d78/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6c5f742673706163653b3d2673706163653b25354373756d5f25374274272537442673706163653b615f253742742c2673706163653b74272537442673706163653b785f2537427427253744\n     :target: https://camo.githubusercontent.com/b9999104eccdcc594abbbef429a3fa49bac27d78/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6c5f742673706163653b3d2673706163653b25354373756d5f25374274272537442673706163653b615f253742742c2673706163653b74272537442673706163653b785f2537427427253744\n     :alt: \n\nInstall\n-------\n\n.. code-block:: bash\n\n   pip install keras-global-self-attention\n\nUsage\n-----\n\n.. code-block:: python\n\n   import keras\n   from keras_global_self_attention import Attention\n\n\n   model = keras.models.Sequential()\n   model.add(keras.layers.Embedding(input_dim=10000,\n                                    output_dim=300,\n                                    mask_zero=True))\n   model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=128,\n                                                          return_sequences=True)))\n   model.add(Attention())\n   model.add(keras.layers.Dense(units=5))\n   model.compile(\n       optimizer='adam',\n       loss='categorical_crossentropy',\n       metrics=['categorical_accuracy'],\n   )\n   model.summary()",
        "description_content_type": "",
        "docs_url": null,
        "download_url": "",
        "downloads": {
            "last_day": -1,
            "last_month": -1,
            "last_week": -1
        },
        "home_page": "https://github.com/CyberZHG/keras-global-self-attention",
        "keywords": "",
        "license": "MIT",
        "maintainer": "",
        "maintainer_email": "",
        "name": "keras-global-self-attention",
        "package_url": "https://pypi.org/project/keras-global-self-attention/",
        "platform": "",
        "project_url": "https://pypi.org/project/keras-global-self-attention/",
        "project_urls": {
            "Homepage": "https://github.com/CyberZHG/keras-global-self-attention"
        },
        "release_url": "https://pypi.org/project/keras-global-self-attention/0.0.7/",
        "requires_dist": null,
        "requires_python": "",
        "summary": "Attention mechanism for processing sequence data that considers the global context for each timestamp",
        "version": "0.0.7"
    },
    "last_serial": 4172178,
    "releases": {
        "0.0.1": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "258d0612c12470f135e43baf0ad0e850",
                    "sha256": "0eb87ac01e8477148af1dcdce5040bd5b865499eaa0601b497a9f5c2b871af27"
                },
                "downloads": -1,
                "filename": "keras-global-self-attention-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "258d0612c12470f135e43baf0ad0e850",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 2138,
                "upload_time": "2018-08-15T04:04:20",
                "url": "https://files.pythonhosted.org/packages/ca/35/f0f8c22651445a96675945879183cc560a14ea98aca292942afb72422910/keras-global-self-attention-0.0.1.tar.gz"
            }
        ],
        "0.0.2": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "a63940b0ade2004f0e9812f1e2bedc8c",
                    "sha256": "0fcf3dbb8faf13ea1817f3a5cfbb91b1cf162e2624b1a3a9acf1f29bb30fc1bf"
                },
                "downloads": -1,
                "filename": "keras-global-self-attention-0.0.2.tar.gz",
                "has_sig": false,
                "md5_digest": "a63940b0ade2004f0e9812f1e2bedc8c",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 2826,
                "upload_time": "2018-08-15T06:36:37",
                "url": "https://files.pythonhosted.org/packages/e4/fb/8732c6a9cc83572e75560dfa1ad3e2923147a15d4028b3f409ad45a5cf30/keras-global-self-attention-0.0.2.tar.gz"
            }
        ],
        "0.0.3": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "44dbc5a66e33243407f1408072e20113",
                    "sha256": "d7d9d12203c37cea87c8d6b19355618b0c5bacf8d740ecf0fe7b5160f288488f"
                },
                "downloads": -1,
                "filename": "keras-global-self-attention-0.0.3.tar.gz",
                "has_sig": false,
                "md5_digest": "44dbc5a66e33243407f1408072e20113",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 2824,
                "upload_time": "2018-08-15T06:41:23",
                "url": "https://files.pythonhosted.org/packages/cf/c5/3b09b0bf41380b0cb94a771eaba75f9c587b73de423ac9f9925f23738fe5/keras-global-self-attention-0.0.3.tar.gz"
            }
        ],
        "0.0.4": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "bfe04081aa41d0597226dc9d943daf12",
                    "sha256": "c8617ad11714863bbce765b2c4416acae2d2f992bc3d78d41d3982a5141edf35"
                },
                "downloads": -1,
                "filename": "keras-global-self-attention-0.0.4.tar.gz",
                "has_sig": false,
                "md5_digest": "bfe04081aa41d0597226dc9d943daf12",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 3038,
                "upload_time": "2018-08-15T07:01:18",
                "url": "https://files.pythonhosted.org/packages/af/ea/5dffa0e6be61eeb6c2a000a829ad994607959142e22f4716954645f5a55a/keras-global-self-attention-0.0.4.tar.gz"
            }
        ],
        "0.0.5": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "229bc4f60a741e1ab6e3a129c2ce5019",
                    "sha256": "a7b4bed8a116de32773d596e0cb61f0abe64baeae75ab991ed40836e0336fa4e"
                },
                "downloads": -1,
                "filename": "keras-global-self-attention-0.0.5.tar.gz",
                "has_sig": false,
                "md5_digest": "229bc4f60a741e1ab6e3a129c2ce5019",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 3182,
                "upload_time": "2018-08-15T07:08:09",
                "url": "https://files.pythonhosted.org/packages/c2/02/e20b262814d13f1b945304652775a59379ba29023c70cf341de1cfe82aa7/keras-global-self-attention-0.0.5.tar.gz"
            }
        ],
        "0.0.6": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "f00af2c39413ebe27c540f8bfdc34d07",
                    "sha256": "0bf995df6b67ee511f58da5f44a95451468d2f9c0f9131c14e56d2de822ebfb6"
                },
                "downloads": -1,
                "filename": "keras-global-self-attention-0.0.6.tar.gz",
                "has_sig": false,
                "md5_digest": "f00af2c39413ebe27c540f8bfdc34d07",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 3172,
                "upload_time": "2018-08-15T07:09:41",
                "url": "https://files.pythonhosted.org/packages/55/b1/feb9b7cbe2cbb67f3cedca9a0040a3c5709b70ac3986fe81a01f9fd08999/keras-global-self-attention-0.0.6.tar.gz"
            }
        ],
        "0.0.7": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "0ec1ea2d433d5f5093d3abc2d699550e",
                    "sha256": "22f96beed7f96b905407e355ad1b125dc57dd566db1b15541cc4ae9f534588bd"
                },
                "downloads": -1,
                "filename": "keras-global-self-attention-0.0.7.tar.gz",
                "has_sig": false,
                "md5_digest": "0ec1ea2d433d5f5093d3abc2d699550e",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 3167,
                "upload_time": "2018-08-15T07:48:15",
                "url": "https://files.pythonhosted.org/packages/e2/be/aa4a7f30a7f039e785c9723e6eb9cf8533c6604d047c7bb5c5ae8d8c60ec/keras-global-self-attention-0.0.7.tar.gz"
            }
        ]
    },
    "urls": [
        {
            "comment_text": "",
            "digests": {
                "md5": "0ec1ea2d433d5f5093d3abc2d699550e",
                "sha256": "22f96beed7f96b905407e355ad1b125dc57dd566db1b15541cc4ae9f534588bd"
            },
            "downloads": -1,
            "filename": "keras-global-self-attention-0.0.7.tar.gz",
            "has_sig": false,
            "md5_digest": "0ec1ea2d433d5f5093d3abc2d699550e",
            "packagetype": "sdist",
            "python_version": "source",
            "requires_python": null,
            "size": 3167,
            "upload_time": "2018-08-15T07:48:15",
            "url": "https://files.pythonhosted.org/packages/e2/be/aa4a7f30a7f039e785c9723e6eb9cf8533c6604d047c7bb5c5ae8d8c60ec/keras-global-self-attention-0.0.7.tar.gz"
        }
    ]
}